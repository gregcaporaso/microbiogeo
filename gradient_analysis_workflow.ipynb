{
 "metadata": {
  "name": "gradient_analysis_workflow"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Define helper functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define various helper functions and their associated tests that will be used in the workflow (and that are not already in QIIME). Run this cell to ensure all required dependencies are installed and setup correctly (e.g. QIIME, numpy) and that all tests pass before running the workflow."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "\n",
      "from numpy import mean, median"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Set up workflow parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure the variables in this section to control how the workflow will be executed. For example, what studies to analyze, categories of interest, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = False\n",
      "\n",
      "if test:\n",
      "    in_dir = 'test_datasets'\n",
      "    out_dir = 'test_gradient_analysis_output'\n",
      "    tree_fp = join('test_datasets', 'overview', 'rep_set.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               'overview': {\n",
      "                            'depths': [50, 100, 146],\n",
      "                            'categories': ['DOB'],\n",
      "                            'subsets': [3, 4],\n",
      "                            'best_method_env_vars': ['DOB']\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis']\n",
      "    methods = {\n",
      "               'mantel': parse_mantel_results,\n",
      "               'mantel_corr': None,\n",
      "               'morans_i': parse_morans_i_results,\n",
      "               'partial_mantel': parse_partial_mantel_results,\n",
      "              }\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 2\n",
      "    num_subsets = 2\n",
      "else:\n",
      "    in_dir = 'datasets'\n",
      "    out_dir = 'gradient_analysis_output'\n",
      "    tree_fp = join('gg_otus_4feb2011', 'trees', 'gg_97_otus_4feb2011.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               '88_soils': {\n",
      "                            'depths': [400, 580, 660],\n",
      "                            'categories': ['LATITUDE', 'PH'],\n",
      "                            'subsets': [10, 20, 30],\n",
      "                            'best_method_env_vars': ['TOT_ORG_CARB', 'SILT_CLAY', 'ELEVATION', 'SOIL_MOISTURE_DEFICIT', 'CARB_NITRO_RATIO',\n",
      "                                                     'ANNUAL_SEASON_TEMP', 'ANNUAL_SEASON_PRECPT', 'PH', 'CMIN_RATE', 'LONGITUDE', 'LATITUDE']\n",
      "                           }, \n",
      "               'glen_canyon': {\n",
      "                               'depths': [15000, 29000, 53000],\n",
      "                               'categories': ['estimated_years_since_submerged_for_plotting'],\n",
      "                               'subsets': [10, 20, 30],\n",
      "                               'best_method_env_vars': ['sample_pH', 'estimated_years_since_submerged_for_plotting', 'Month', 'Day', 'Year',\n",
      "                                                        'days_since_epoch', 'Hour', 'Replicate', 'DNA.I.D.No.']\n",
      "                              },\n",
      "               'keyboard': {\n",
      "                            'depths': [390, 780, 1015],\n",
      "                            'categories': [],\n",
      "                            'subsets': [10, 20, 30],\n",
      "                            'best_method_env_vars': []\n",
      "                           }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis', 'weighted_unifrac', 'unweighted_unifrac']\n",
      "    methods = {\n",
      "               'mantel': parse_mantel_results,\n",
      "               'mantel_corr': None,\n",
      "               'morans_i': parse_morans_i_results,\n",
      "               'partial_mantel': parse_partial_mantel_results\n",
      "              }\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 5\n",
      "    num_subsets = 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Generate distance matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate distance matrices for each study at even sampling depths that exclude 5%, 25%, and 50% of the samples from the input OTU table (these numbers were calculated beforehand). Generate Euclidean, Bray-Curtis, weighted UniFrac, and unweighted UniFrac distance matrices at each sampling depth using the GreenGenes 97% tree. Also generate several shuffled versions of each distance matrix, which can be used later as negative controls.\n",
      "\n",
      "In addition, generate several subsets of each distance matrix with the specified number of samples, which can be used later to test how the methods perform on different study sizes.\n",
      "\n",
      "The keyboard study is a bit of an exception in that we only want to create a distance matrix that includes samples taken directly from keys (not human subject fingertips) because we want to see if keys that are closer to each other are correlated with community similarity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The parameters need to be wrapped in parens in order to work with map.\n",
      "def generate_per_study_depth_dms((study, depth, metrics, categories, subsets, num_shuffled, num_subsets, in_dir, out_dir, tree_fp, shuffle_dm_fn,\n",
      "                                  pick_dm_subset_fn)):\n",
      "    from os.path import join\n",
      "    \n",
      "    in_study_dir = join(in_dir, study)\n",
      "    out_study_dir = join(out_dir, study)\n",
      "    !mkdir $out_study_dir\n",
      "    !cp $in_study_dir/map.txt $out_study_dir/\n",
      "    map_fp = join(out_study_dir, 'map.txt')\n",
      "    \n",
      "    full_otu_fp = join(in_study_dir, 'otu_table.biom')\n",
      "    even_otu_fp = join(out_study_dir, 'otu_table_even%d.biom' % depth)\n",
      "    bdiv_out_dir = join(out_study_dir, 'bdiv_even%d' % depth)\n",
      "    \n",
      "    metrics_param = ','.join(metrics)\n",
      "    !single_rarefaction.py -i $full_otu_fp -o $even_otu_fp -d $depth\n",
      "    !beta_diversity.py -i $even_otu_fp -o $bdiv_out_dir -m $metrics_param -t $tree_fp\n",
      "    \n",
      "    # Rename each file to match QIIME's standard naming conventions of distance matrices. Generate shuffled versions of each distance matrix.\n",
      "    for metric in metrics:\n",
      "        dm_fp = join(bdiv_out_dir, '%s_otu_table_even%d.txt' % (metric, depth))\n",
      "        renamed_dm_fp = join(bdiv_out_dir, '%s_dm.txt' % metric)\n",
      "        \n",
      "        # Filter the keyboard study distance matrix to include only samples taken from keys of subjects M2, M3, and M9.\n",
      "        if study == 'keyboard':\n",
      "            !filter_distance_matrix.py -i $dm_fp -o $renamed_dm_fp -m $map_fp -s 'COMMON_NAME:keyboard;HOST_SUBJECT_ID:M2,M3,M9'\n",
      "            !rm $dm_fp\n",
      "        else:\n",
      "            !mv $dm_fp $renamed_dm_fp\n",
      "        \n",
      "        for i in range(1, num_shuffled + 1):\n",
      "            renamed_dm_f = open(renamed_dm_fp, 'U')\n",
      "            shuffled_dm_fp = join(bdiv_out_dir, '%s_dm_shuffled%d.txt' % (metric, i))\n",
      "            shuffled_dm_f = open(shuffled_dm_fp, 'w')\n",
      "            shuffled_dm_f.write(shuffle_dm_fn(renamed_dm_f))\n",
      "            shuffled_dm_f.close()\n",
      "            renamed_dm_f.close()\n",
      "        \n",
      "        # Create subsets of each non-shuffled distance matrix.\n",
      "        for subset in subsets:\n",
      "            for i in range(1, num_subsets + 1):\n",
      "                    subset_dm_fp = join(bdiv_out_dir, '%s_dm_n%d_%d.txt' % (metric, subset, i))\n",
      "                    subset_dm = open(subset_dm_fp, 'w')\n",
      "                    subset_dm.write(pick_dm_subset_fn(open(renamed_dm_fp, 'U'), subset))\n",
      "                    subset_dm.close()\n",
      "            \n",
      "    # Create distance matrices from environmental variables in mapping file. These are independent of sampling depth and metric, so we only need to create them\n",
      "    # once for each study. Again, keyboard is unique in that we cannot easily create a key distance matrix from the mapping file. We'll use one that has been\n",
      "    # precalculated.\n",
      "    for category in categories:\n",
      "        env_dm_out_dir = join(out_study_dir, '%s_dm' % category)\n",
      "        env_dm_fp = join(env_dm_out_dir, '%s.txt' % category)\n",
      "        renamed_env_dm_fp = join(out_study_dir, '%s_dm.txt' % category)\n",
      "        !distance_matrix_from_mapping.py -i $map_fp -c $category -o $env_dm_out_dir\n",
      "        !mv $env_dm_fp $renamed_env_dm_fp\n",
      "        !rm -rf $env_dm_out_dir  \n",
      "    \n",
      "    if study == 'keyboard':\n",
      "        key_dm_fp = join(in_study_dir, 'euclidean_key_distances_dm.txt')\n",
      "        !cp $key_dm_fp $out_study_dir/\n",
      "        indiv_dm_fp = join(in_study_dir, 'median_unifrac_individual_distances_dm.txt')\n",
      "        !cp $indiv_dm_fp $out_study_dir/\n",
      "        \n",
      "# Process each depth in each study in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "!mkdir $out_dir\n",
      "per_study_depths = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        per_study_depths.append((study, depth, metrics, studies[study]['categories'], studies[study]['subsets'], num_shuffled, num_subsets,\n",
      "                                 in_dir, out_dir, tree_fp, shuffle_dm, pick_dm_subset))\n",
      "out = dview.map(generate_per_study_depth_dms, per_study_depths)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run gradient analysis methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run each *gradient analysis* statistical method on each distance matrix. These are methods that test out continuous variables such as pH, latitude, etc.. Not all methods can be tested on the same variables/inputs. For example, Moran's I cannot be tested on the keyboard study's key distances because it is univariate, and partial Mantel can only be tested on the keyboard study because that is the only study we have a control matrix for."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import basename, exists, join, splitext\n",
      "from random import shuffle\n",
      "\n",
      "def run_command(cmd):\n",
      "    !$cmd\n",
      "\n",
      "jobs = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        for method in methods:\n",
      "            study_dir = join(out_dir, study)\n",
      "            map_fp = join(study_dir, 'map.txt')\n",
      "            depth_dir = join(study_dir, 'bdiv_even%d' % depth)\n",
      "            dm_wildcard = join(depth_dir, '*_dm*.txt')\n",
      "            dm_fps = !ls $dm_wildcard\n",
      "            \n",
      "            for dm_fp in dm_fps:\n",
      "                for category in studies[study]['categories']:\n",
      "                    for permutation in permutations:\n",
      "                        if method == 'mantel' or method == 'mantel_corr':\n",
      "                            in_dm_fps = '%s,%s' % (dm_fp, join(study_dir, '%s_dm.txt' % category))\n",
      "                            results_dir = join(depth_dir, '%s_%s_%s_%d' % (splitext(basename(dm_fp))[0], method, category, permutation))\n",
      "                            \n",
      "                            # Skip the job if the results dir exists and is not empty. We'll assume it was created from a previous run.\n",
      "                            if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                                jobs.append('compare_distance_matrices.py --method %s -n %d -i %s -o %s' % (method, permutation, in_dm_fps, results_dir))\n",
      "                    \n",
      "                    # Moran's I does not accept a number of permutations.\n",
      "                    if method == 'morans_i':\n",
      "                        results_dir = join(depth_dir, '%s_%s_%s' % (splitext(basename(dm_fp))[0], method, category))\n",
      "                        \n",
      "                        if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                            jobs.append('compare_categories.py --method %s -i %s -m %s -c %s -o %s' % (method, dm_fp, map_fp, category, results_dir))\n",
      "        \n",
      "            if study == 'keyboard':\n",
      "                for dm_fp in dm_fps:\n",
      "                    for permutation in permutations:\n",
      "                        in_dm_fps = '%s,%s' % (dm_fp, join(study_dir, 'euclidean_key_distances_dm.txt'))\n",
      "                        \n",
      "                        if method == 'mantel' or method == 'mantel_corr':\n",
      "                            results_dir = join(depth_dir, '%s_%s_%s_%d' % (splitext(basename(dm_fp))[0], method, 'key_distance', permutation))\n",
      "                            \n",
      "                            if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                                jobs.append('compare_distance_matrices.py --method %s -n %d -i %s -o %s' % (method, permutation, in_dm_fps, results_dir))\n",
      "                        elif method == 'partial_mantel':\n",
      "                            control_dm_fp = join(study_dir, 'median_unifrac_individual_distances_dm.txt')\n",
      "                            results_dir = join(depth_dir, '%s_%s_%s_%d' % (splitext(basename(dm_fp))[0], method, 'key_distance', permutation))\n",
      "                            \n",
      "                            if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                                jobs.append('compare_distance_matrices.py --method %s -n %d -i %s -o %s -c %s' % (method, permutation, in_dm_fps, results_dir, control_dm_fp))\n",
      "\n",
      "# Process each script run in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "shuffle(jobs)\n",
      "out = dview.map(run_command, jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run BEST method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "BEST is very different from the rest of the methods in terms of input and output, so run it separately here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_per_study_depth_best_analysis((study, depth, env_vars, out_dir)):\n",
      "    from os.path import basename, join, splitext\n",
      "    \n",
      "    if len(env_vars) > 0:\n",
      "        study_dir = join(out_dir, study)\n",
      "        map_fp = join(study_dir, 'map.txt')\n",
      "        depth_dir = join(study_dir, 'bdiv_even%d' % depth)\n",
      "        dm_wildcard = join(depth_dir, '*_dm*.txt')\n",
      "        dm_fps = !ls $dm_wildcard\n",
      "        env_vars_str = ','.join(env_vars)\n",
      "        \n",
      "        for dm_fp in dm_fps:\n",
      "            results_dir = join(depth_dir, '%s_%s' % (splitext(basename(dm_fp))[0], 'best'))\n",
      "            !compare_categories.py --method 'best' -i $dm_fp -m $map_fp -c $env_vars_str -o $results_dir\n",
      "\n",
      "# Process BEST at each depth in each study in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "per_study_depths = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        per_study_depths.append((study, depth, studies[study]['best_method_env_vars'], out_dir))\n",
      "out = dview.map(run_per_study_depth_best_analysis, per_study_depths)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Collate results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parse and collect the effect size statistics and p-values from each of the tests that were run. The resulting data structure can then be used to format result tables.\n",
      "\n",
      "Mantel correlogram is hard to summarize because it produces a correlogram, and many Mantel statistics for each distance class. We'll need to look at those results by hand and summarize them in the paper. The same holds for BEST: though it does not create a visual plot, it does not provide p-values. It mainly tells you which environmental variables best correlate with the community data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = {}\n",
      "for depth_idx, depth_desc in enumerate(depth_descs):\n",
      "    depth_res = {}\n",
      "    for metric in metrics:\n",
      "        metric_res = {}\n",
      "        for method, res_parsing_fn in methods.items():\n",
      "            if method == 'mantel_corr':\n",
      "                # Completely ignore Mantel correlogram (for now at least).\n",
      "                continue\n",
      "                \n",
      "            method_res = {}\n",
      "            for study in studies:\n",
      "                study_res = {}\n",
      "                \n",
      "                # Figure out what our actual depth is for the study and what distance matrix subsets we used.\n",
      "                depth = studies[study]['depths'][depth_idx]\n",
      "                subsets = studies[study]['subsets']\n",
      "                \n",
      "                for category in studies[study]['categories']:\n",
      "                    category_res = {}\n",
      "                    \n",
      "                    if method == 'mantel' or method == 'morans_i':\n",
      "                        full_ess = []\n",
      "                        full_p_vals = []\n",
      "                        shuff_ess = []\n",
      "                        shuff_p_vals = []\n",
      "                        \n",
      "                        if method == 'mantel':\n",
      "                            # TODO This next part is pretty messy... it needs to get cleaned up.\n",
      "                            for permutation in permutations:\n",
      "                                # Collect results for full distance matrices.\n",
      "                                full_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_%s_%s_%d' % (metric, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                                full_es, full_p_val = res_parsing_fn(full_res_f)\n",
      "                                full_res_f.close()\n",
      "                                full_ess.append(full_es)\n",
      "                                full_p_vals.append(full_p_val)\n",
      "                                \n",
      "                                # Collect results for shuffled distance matrices.\n",
      "                                avg_shuff_ess = []\n",
      "                                avg_shuff_p_vals = []\n",
      "                                for shuff_num in range(1, num_shuffled + 1):\n",
      "                                    shuff_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_shuffled%d_%s_%s_%d' % (metric, shuff_num, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                                    shuff_es, shuff_p_val = res_parsing_fn(shuff_res_f)\n",
      "                                    shuff_res_f.close()\n",
      "                                    avg_shuff_ess.append(shuff_es)\n",
      "                                    avg_shuff_p_vals.append(shuff_p_val)\n",
      "                                shuff_ess.append(median(avg_shuff_ess))\n",
      "                                shuff_p_vals.append(median(avg_shuff_p_vals))\n",
      "                        elif method == 'morans_i':\n",
      "                            full_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_%s_%s' % (metric, method, category), '%s_results.txt' % method), 'U')\n",
      "                            full_es, full_p_val = res_parsing_fn(full_res_f)\n",
      "                            full_res_f.close()\n",
      "                            full_ess.append(full_es)\n",
      "                            full_p_vals.append(full_p_val)\n",
      "                            \n",
      "                            avg_shuff_ess = []\n",
      "                            avg_shuff_p_vals = []\n",
      "                            for shuff_num in range(1, num_shuffled + 1):\n",
      "                                shuff_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_shuffled%d_%s_%s' % (metric, shuff_num, method, category), '%s_results.txt' % method), 'U')\n",
      "                                shuff_es, shuff_p_val = res_parsing_fn(shuff_res_f)\n",
      "                                shuff_res_f.close()\n",
      "                                avg_shuff_ess.append(shuff_es)\n",
      "                                avg_shuff_p_vals.append(shuff_p_val)\n",
      "                            shuff_ess.append(median(avg_shuff_ess))\n",
      "                            shuff_p_vals.append(median(avg_shuff_p_vals))\n",
      "                                \n",
      "                        if len(set(full_ess)) != 1 or len(set(shuff_ess)) != 1:\n",
      "                            raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                        for p_val in full_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        for p_val in shuff_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        category_res['full'] = (full_ess[0], full_p_vals)\n",
      "                        category_res['shuffled'] = (shuff_ess[0], shuff_p_vals)\n",
      "                            \n",
      "                        # Collect results for distance matrix subsets.\n",
      "                        ss_ess = []\n",
      "                        ss_p_vals = []\n",
      "                        for subset in subsets:\n",
      "                            gs_ess = []\n",
      "                            gs_p_vals = []\n",
      "                            \n",
      "                            if method == 'mantel':\n",
      "                                for permutation in permutations:\n",
      "                                    avg_ss_ess = []\n",
      "                                    avg_ss_p_vals = []\n",
      "                                    for ss_num in range(1, num_subsets + 1):\n",
      "                                        ss_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_n%d_%d_%s_%s_%d' % (metric, subset, ss_num, method, category, permutation),\n",
      "                                                             '%s_results.txt' % method), 'U')\n",
      "                                        ss_es, ss_p_val = res_parsing_fn(ss_res_f)\n",
      "                                        ss_res_f.close()\n",
      "                                        avg_ss_ess.append(ss_es)\n",
      "                                        avg_ss_p_vals.append(ss_p_val)\n",
      "                                    gs_ess.append(median(avg_ss_ess))\n",
      "                                    gs_p_vals.append(median(avg_ss_p_vals))\n",
      "                            elif method == 'morans_i':\n",
      "                                avg_ss_ess = []\n",
      "                                avg_ss_p_vals = []\n",
      "                                for ss_num in range(1, num_subsets + 1):\n",
      "                                    ss_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_n%d_%d_%s_%s' % (metric, subset, ss_num, method, category),\n",
      "                                                         '%s_results.txt' % method), 'U')\n",
      "                                    ss_es, ss_p_val = res_parsing_fn(ss_res_f)\n",
      "                                    ss_res_f.close()\n",
      "                                    avg_ss_ess.append(ss_es)\n",
      "                                    avg_ss_p_vals.append(ss_p_val)\n",
      "                                gs_ess.append(median(avg_ss_ess))\n",
      "                                gs_p_vals.append(median(avg_ss_p_vals))\n",
      "                            \n",
      "                            if len(set(gs_ess)) != 1:\n",
      "                                raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                            for p_val in gs_p_vals:\n",
      "                                if p_val < 0 or p_val > 1:\n",
      "                                    raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                            ss_ess.append(gs_ess[0])\n",
      "                            ss_p_vals.append(gs_p_vals)\n",
      "                        \n",
      "                        if len(ss_ess) != len(ss_p_vals):\n",
      "                            raise ValueError(\"We don't have the same number of effect size statistics as p-values. Something went wrong...\")\n",
      "                        category_res['subsampled'] = (ss_ess, ss_p_vals)\n",
      "                    \n",
      "                    # Add the category results. Will be an empty dictionary if the method (such as partial Mantel) was not applicable.\n",
      "                    study_res[category] = category_res\n",
      "                \n",
      "                if study == 'keyboard':\n",
      "                    category = 'key_distance'\n",
      "                    category_res = {}\n",
      "                    \n",
      "                    if method == 'mantel' or method == 'partial_mantel':\n",
      "                        full_ess = []\n",
      "                        full_p_vals = []\n",
      "                        shuff_ess = []\n",
      "                        shuff_p_vals = []\n",
      "                        \n",
      "                        for permutation in permutations:\n",
      "                            full_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_%s_%s_%d' % (metric, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                            full_es, full_p_val = res_parsing_fn(full_res_f)\n",
      "                            full_res_f.close()\n",
      "                            full_ess.append(full_es)\n",
      "                            full_p_vals.append(full_p_val)\n",
      "                            \n",
      "                            avg_shuff_ess = []\n",
      "                            avg_shuff_p_vals = []\n",
      "                            for shuff_num in range(1, num_shuffled + 1):\n",
      "                                shuff_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_shuffled%d_%s_%s_%d' % (metric, shuff_num, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                                shuff_es, shuff_p_val = res_parsing_fn(shuff_res_f)\n",
      "                                shuff_res_f.close()\n",
      "                                avg_shuff_ess.append(shuff_es)\n",
      "                                avg_shuff_p_vals.append(shuff_p_val)\n",
      "                            shuff_ess.append(median(avg_shuff_ess))\n",
      "                            shuff_p_vals.append(median(avg_shuff_p_vals))\n",
      "                            \n",
      "                        if len(set(full_ess)) != 1 or len(set(shuff_ess)) != 1:\n",
      "                            raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                        for p_val in full_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        for p_val in shuff_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        category_res['full'] = (full_ess[0], full_p_vals)\n",
      "                        category_res['shuffled'] = (shuff_ess[0], shuff_p_vals)\n",
      "                        \n",
      "                        # Collect results for distance matrix subsets.\n",
      "                        ss_ess = []\n",
      "                        ss_p_vals = []\n",
      "                        for subset in subsets:\n",
      "                            gs_ess = []\n",
      "                            gs_p_vals = []\n",
      "                            \n",
      "                            for permutation in permutations:\n",
      "                                avg_ss_ess = []\n",
      "                                avg_ss_p_vals = []\n",
      "                                for ss_num in range(1, num_subsets + 1):\n",
      "                                    ss_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_n%d_%d_%s_%s_%d' % (metric, subset, ss_num, method, category, permutation),\n",
      "                                                         '%s_results.txt' % method), 'U')\n",
      "                                    ss_es, ss_p_val = res_parsing_fn(ss_res_f)\n",
      "                                    ss_res_f.close()\n",
      "                                    avg_ss_ess.append(ss_es)\n",
      "                                    avg_ss_p_vals.append(ss_p_val)\n",
      "                                gs_ess.append(median(avg_ss_ess))\n",
      "                                gs_p_vals.append(median(avg_ss_p_vals))\n",
      "                        \n",
      "                            if len(set(gs_ess)) != 1:\n",
      "                                raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                            for p_val in gs_p_vals:\n",
      "                                if p_val < 0 or p_val > 1:\n",
      "                                    raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                            ss_ess.append(gs_ess[0])\n",
      "                            ss_p_vals.append(gs_p_vals)\n",
      "                    \n",
      "                        if len(ss_ess) != len(ss_p_vals):\n",
      "                            raise ValueError(\"We don't have the same number of effect size statistics as p-values. Something went wrong...\")\n",
      "                        category_res['subsampled'] = (ss_ess, ss_p_vals)\n",
      "                        \n",
      "                    study_res[category] = category_res\n",
      "                method_res[study] = study_res\n",
      "            metric_res[method] = method_res\n",
      "        depth_res[metric] = metric_res\n",
      "    results[depth_desc] = depth_res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gradient subset testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specifically test Mantel and Moran's I on subsets of the original distance matrix. This time the subsets will not be randomly chosen, but instead samples will be chosen along the gradient for each subset (which is what a researcher might do instead of randomly picking samples in order to test a gradient). A plot will be generated with subset size on the x-axis and test statistic on the y-axis. This will allow us to see if there is a cutoff/threshold for gradient detection based on the number of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "from math import ceil\n",
      "from os.path import join\n",
      "from random import randint, shuffle\n",
      "from matplotlib.pyplot import errorbar, figure, legend, title, xlabel, xlim, ylabel\n",
      "from numpy import mean, std\n",
      "from qiime.filter import filter_samples_from_distance_matrix\n",
      "from qiime.parse import parse_distmat, parse_mapping_file_to_dict\n",
      "\n",
      "def run_cmd(cmd):\n",
      "    !$cmd\n",
      "\n",
      "samp_sizes = [5, 10, 20, 40, 60, 80]\n",
      "num_subsets = 10\n",
      "study = '88_soils'\n",
      "gradients = {'PH': ['b', 'pH'], 'LATITUDE': ['r', 'Latitude']}\n",
      "even_depth = 400\n",
      "out_dir = '%s_gradient_analysis' % study\n",
      "methods = {\n",
      "           'mantel': parse_mantel_results,\n",
      "           'morans_i': parse_morans_i_results\n",
      "          }\n",
      "map_fp = 'gradient_analysis_output/%s/map.txt' % study\n",
      "num_perms = 999"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdm, _ = parse_mapping_file_to_dict(open(map_fp, 'U'))\n",
      "dm_labels, dm_data = parse_distmat(open('gradient_analysis_output/%s/bdiv_even%d/unweighted_unifrac_dm.txt' % (study, even_depth), 'U'))\n",
      "!mkdir $out_dir\n",
      "\n",
      "cmds = []\n",
      "for gradient, plot_options in gradients.items():\n",
      "    # Only keep the sample IDs that are in both the mapping file and distance matrix.\n",
      "    samp_ids = [(samp_id, float(metadata[gradient])) for samp_id, metadata in mdm.items() if samp_id in dm_labels]\n",
      "    samp_ids.sort(key=lambda samp_id: samp_id[1])\n",
      "    \n",
      "    for samp_size in samp_sizes:\n",
      "        # Adapted from http://stackoverflow.com/a/9873935\n",
      "        # We add 1 to the number of samples we want because we want samp_size intervals to choose from.\n",
      "        bin_idxs = [int(ceil(i * len(samp_ids) / (samp_size + 1))) for i in range(samp_size + 1)]\n",
      "        \n",
      "        for subset_num in range(1, num_subsets + 1):\n",
      "            samp_ids_to_keep = []\n",
      "            for i in range(len(bin_idxs) - 1):\n",
      "                if i == len(bin_idxs) - 2:\n",
      "                    # We're at the last bin, so choose from the entire bin range.\n",
      "                    samp_ids_to_keep.append(samp_ids[randint(bin_idxs[i], bin_idxs[i + 1])][0])\n",
      "                else:\n",
      "                    # We subtract 1 since randint is inclusive on both sides, and we don't want to choose\n",
      "                    # the same sample ID multiple times from different bins.\n",
      "                    samp_ids_to_keep.append(samp_ids[randint(bin_idxs[i], bin_idxs[i + 1] - 1)][0])\n",
      "            assert len(samp_ids_to_keep) == samp_size, \"%d != %d\" % (len(samp_ids_to_keep), samp_size)\n",
      "            \n",
      "            subset_dm_fp = join(out_dir, 'unweighted_unifrac_dm_%s_size_%d_%d.txt' % (gradient, samp_size, subset_num))\n",
      "            subset_dm_f = open(subset_dm_fp, 'w')\n",
      "            subset_dm_f.write(filter_samples_from_distance_matrix((dm_labels, dm_data), samp_ids_to_keep, negate=True))\n",
      "            subset_dm_f.close()\n",
      "            \n",
      "            # Build Mantel command.\n",
      "            in_dm_fps = '%s,%s' % (subset_dm_fp, join('gradient_analysis_output/%s/%s_dm.txt' % (study, gradient)))\n",
      "            results_dir = join(out_dir, 'unweighted_unifrac_dm_%s_size_%d_%d_mantel' % (gradient, samp_size, subset_num))\n",
      "            cmds.append('compare_distance_matrices.py --method mantel -n %d -i %s -o %s' % (num_perms, in_dm_fps, results_dir))\n",
      "            \n",
      "            # Build Moran's I command.\n",
      "            results_dir = join(out_dir, 'unweighted_unifrac_dm_%s_size_%d_%d_morans_i' % (gradient, samp_size, subset_num))\n",
      "            cmds.append('compare_categories.py --method morans_i -i %s -m %s -c %s -o %s' % (subset_dm_fp, map_fp, gradient, results_dir))\n",
      "\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "shuffle(cmds)\n",
      "out = dview.map(run_cmd, cmds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for method, parse_fn in methods.items():\n",
      "    # Twin y-axis code is based on http://matplotlib.org/examples/api/two_scales.html\n",
      "    fig = figure()\n",
      "    ax1 = fig.add_subplot(111)\n",
      "    ax2 = ax1.twinx()\n",
      "    \n",
      "    for gradient, plot_options in gradients.items():\n",
      "        avg_test_stats = []\n",
      "        std_test_stats = []\n",
      "        avg_p_vals = []\n",
      "        std_p_vals = []\n",
      "        \n",
      "        for samp_size in samp_sizes:\n",
      "            test_stats = []\n",
      "            p_vals = []\n",
      "            \n",
      "            for subset_num in range(1, num_subsets + 1):\n",
      "                results_dir = join(out_dir, 'unweighted_unifrac_dm_%s_size_%d_%d_%s' % (gradient, samp_size, subset_num, method))\n",
      "                test_stat, p_val = parse_fn(open(join(results_dir, '%s_results.txt' % method), 'U'))\n",
      "                test_stats.append(test_stat)\n",
      "                p_vals.append(p_val)\n",
      "                \n",
      "            avg_test_stats.append(mean(test_stats))\n",
      "            std_test_stats.append(std(test_stats))\n",
      "            avg_p_vals.append(mean(p_vals))\n",
      "            std_p_vals.append(std(p_vals))\n",
      "        \n",
      "        # Plot test statistics on left axis.\n",
      "        ax1.errorbar(samp_sizes, avg_test_stats, yerr=std_test_stats, color=plot_options[0], label=plot_options[1], fmt='-')\n",
      "        \n",
      "        # Plot p-values on the right axis.\n",
      "        ax2.errorbar(samp_sizes, avg_p_vals, yerr=std_p_vals, color=plot_options[0], label=plot_options[1], fmt='-', linestyle='--')\n",
      "    \n",
      "    xlim(0, 85)\n",
      "    #ax2.set_ylim(0.0, 1.0)\n",
      "    title('%s: %s' % (study, method))\n",
      "    ax1.set_xlabel('Number of samples')\n",
      "    ax1.set_ylabel('Average test statistic with standard deviation')\n",
      "    ax2.set_ylabel('Average p-value with standard deviation')\n",
      "    \n",
      "    #lines, labels = ax1.get_legend_handles_labels()\n",
      "    #lines2, labels2 = ax2.get_legend_handles_labels()\n",
      "    #ax2.legend(lines + lines2, labels + labels2)\n",
      "    legend()\n",
      "    fig.savefig(join(out_dir, 'gradient_analysis_plot_%s_%s.pdf' % (study, method)), format='pdf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Method Comparison Heatmap"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a heatmap comparing every pair of *gradient analysis* methods using Pearson or Spearman correlation. For data, we will use all results of the original run of all even sampling depths, metrics, and datasets that match between the pair of methods."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "from matplotlib.pyplot import colorbar, figure, imshow, matshow, savefig, subplot, tight_layout, xticks, yticks\n",
      "from numpy import ones\n",
      "from cogent.maths.stats.test import pearson, spearman\n",
      "\n",
      "heatmap_methods = ['mantel', 'morans_i']\n",
      "heatmap_method_labels = ['Mantel', 'Moran\\'s I']\n",
      "\n",
      "# Gather all test statistics for each method (in the same order for each method!).\n",
      "method_data = defaultdict(list)\n",
      "for depth_desc, depth_res in results.items():\n",
      "    for metric, metric_res in depth_res.items():\n",
      "        for method, method_res in metric_res.items():\n",
      "            if method in heatmap_methods:\n",
      "                for study, study_res in sorted(method_res.items()):\n",
      "                    # Can't compare Moran's I and Mantel using keyboard study because we can't run\n",
      "                    # Moran's I over that dataset.\n",
      "                    if study != 'keyboard':\n",
      "                        for category, category_res in sorted(study_res.items()):\n",
      "                            method_data[method].append(category_res['full'][0])\n",
      "                            method_data[method].append(category_res['shuffled'][0])\n",
      "                            \n",
      "                            for es, p_vals in zip(*category_res['subsampled']):\n",
      "                                method_data[method].append(es)\n",
      "\n",
      "# Make sure our data looks sane... we should have the same number of observations for each method.\n",
      "data_length = None\n",
      "for method, data in method_data.items():\n",
      "    if data_length is None:\n",
      "        data_length = len(data)\n",
      "    elif len(data) != data_length:\n",
      "        raise ValueError(\"The number of test statistics is not the same between all methods, so we can't compare them! Something went wrong...\")\n",
      "\n",
      "# Compute the correlation coefficient between each pair of methods and put the output in an array. This array can then be used to generate a\n",
      "# text-based table or heatmap.\n",
      "for correlation_name, correlation_fn in ('pearson', pearson), ('spearman', spearman):\n",
      "    num_methods = len(heatmap_methods)\n",
      "    heatmap_data = ones((num_methods, num_methods))\n",
      "    \n",
      "    # I know this is inefficient, but it really doesn't matter for what we're doing here...\n",
      "    for method1_idx, method1 in enumerate(heatmap_methods):\n",
      "        for method2_idx, method2 in enumerate(heatmap_methods):\n",
      "            corr_coeff = correlation_fn(method_data[method1], method_data[method2])\n",
      "            heatmap_data[method1_idx][method2_idx] = corr_coeff\n",
      "            \n",
      "    # Generate the heatmap. Code based on http://matplotlib.org/users/tight_layout_guide.html and\n",
      "    # http://psaffrey.wordpress.com/2010/07/05/chromosome-interactions-heatmaps-and-matplotlib/\n",
      "    fig = figure()\n",
      "    ax = subplot(111)\n",
      "    im = ax.matshow(heatmap_data)\n",
      "    colorbar(im, use_gridspec=True)\n",
      "    xticks(range(len(heatmap_method_labels)), heatmap_method_labels, rotation=90)\n",
      "    yticks(range(len(heatmap_method_labels)), heatmap_method_labels)\n",
      "    tight_layout()\n",
      "    savefig('gradient_analysis_heatmap_%s.pdf' % correlation_name, format='pdf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tight_layout : falling back to Agg renderer"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tight_layout : falling back to Agg renderer"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    }
   ],
   "metadata": {}
  }
 ]
}