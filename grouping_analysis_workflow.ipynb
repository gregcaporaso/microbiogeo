{
 "metadata": {
  "name": "grouping_analysis_workflow"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Define helper functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define various helper functions and their associated tests that will be used in the workflow (and that are not already in QIIME). Run this cell to ensure all required dependencies are installed and setup correctly (e.g. QIIME, numpy) and that all tests pass before running the workflow."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "\n",
      "from numpy import mean, median"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Set up workflow parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure the variables in this section to control how the workflow will be executed. For example, what studies to analyze, categories of interest, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = False\n",
      "\n",
      "if test:\n",
      "    in_dir = 'test_datasets'\n",
      "    out_dir = 'test_grouping_analysis_output'\n",
      "    tree_fp = join('test_datasets', 'overview', 'rep_set.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               'overview': {\n",
      "                            'depths': [50, 100, 146],\n",
      "                            'categories': ['Treatment'],\n",
      "                            'group_sizes': [3, 4]\n",
      "                           },\n",
      "               'overview2': {\n",
      "                             'depths': [50, 100, 146],\n",
      "                             'categories': ['Treatment'],\n",
      "                             'group_sizes': [3, 4]\n",
      "                            }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis']\n",
      "    grouping_methods = {\n",
      "                        'adonis': parse_adonis_results,\n",
      "                        'anosim': parse_anosim_permanova_results\n",
      "                       }\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 2\n",
      "    num_subsampled = 2\n",
      "else:\n",
      "    in_dir = 'datasets'\n",
      "    out_dir = 'grouping_analysis_output'\n",
      "    tree_fp = join('gg_otus_4feb2011', 'trees', 'gg_97_otus_4feb2011.tre')\n",
      "    depth_descs = ['5_percent', '25_percent', '50_percent']\n",
      "    studies = {\n",
      "               '88_soils': {\n",
      "                            'depths': [400, 580, 660],\n",
      "                            'categories': ['ENV_BIOME'],\n",
      "                            'group_sizes': [5, 10, 20, 40]\n",
      "                           }, \n",
      "               'glen_canyon': {\n",
      "                               'depths': [15000, 29000, 53000],\n",
      "                               'categories': ['CurrentlyWet'],\n",
      "                               'group_sizes': [5, 10, 20, 40]\n",
      "                              },\n",
      "               'keyboard': {\n",
      "                            'depths': [390, 780, 1015],\n",
      "                            'categories': ['HOST_SUBJECT_ID'],\n",
      "                            'group_sizes': [5, 10, 20, 40]\n",
      "                           },\n",
      "               'whole_body': {\n",
      "                              'depths': [575, 877, 1110],\n",
      "                              'categories': ['BODY_SITE', 'SEX'],\n",
      "                              'group_sizes': [5, 10, 20, 40]\n",
      "                             }\n",
      "              }\n",
      "    metrics = ['euclidean', 'bray_curtis', 'weighted_unifrac', 'unweighted_unifrac']\n",
      "    grouping_methods = {\n",
      "                        'adonis': parse_adonis_results,\n",
      "                        'anosim': parse_anosim_permanova_results,\n",
      "                        'mrpp': parse_mrpp_results,\n",
      "                        'permanova': parse_anosim_permanova_results,\n",
      "                        'dbrda': parse_dbrda_results,\n",
      "                        'permdisp': parse_permdisp_results\n",
      "                       }\n",
      "    permutations = [99, 999]\n",
      "    num_shuffled = 5\n",
      "    num_subsampled = 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Generate distance matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate distance matrices for each study at even sampling depths that exclude 5%, 25%, and 50% of the samples from the input OTU table (these numbers were calculated beforehand). Generate Euclidean, Bray-Curtis, weighted UniFrac, and unweighted UniFrac distance matrices at each sampling depth using the GreenGenes 97% tree. Also generate several shuffled versions of each distance matrix, which can be used later as negative controls.\n",
      "\n",
      "In addition, generate several subsampled versions of each distance matrix to control the size of each group of samples (based on a category), which can be used later to test how the methods perform on different study sizes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# The parameters need to be wrapped in parens in order to work with map.\n",
      "def generate_per_study_depth_dms((study, depth, metrics, categories, group_sizes, num_shuffled, num_subsampled, in_dir, out_dir, tree_fp, shuffle_dm_fn,\n",
      "                                  subsample_dm_fn)):\n",
      "    from os.path import join\n",
      "    \n",
      "    in_study_dir = join(in_dir, study)\n",
      "    out_study_dir = join(out_dir, study)\n",
      "    !mkdir $out_study_dir\n",
      "    !cp $in_study_dir/map.txt $out_study_dir/\n",
      "    map_fp = join(out_study_dir, 'map.txt')\n",
      "    \n",
      "    full_otu_fp = join(in_study_dir, 'otu_table.biom')\n",
      "    even_otu_fp = join(out_study_dir, 'otu_table_even%d.biom' % depth)\n",
      "    bdiv_out_dir = join(out_study_dir, 'bdiv_even%d' % depth)\n",
      "    \n",
      "    metrics_param = ','.join(metrics)\n",
      "    !single_rarefaction.py -i $full_otu_fp -o $even_otu_fp -d $depth\n",
      "    !beta_diversity.py -i $even_otu_fp -o $bdiv_out_dir -m $metrics_param -t $tree_fp\n",
      "    \n",
      "    # Rename each file to match QIIME's standard naming conventions of distance matrices. Generate shuffled versions of each distance matrix.\n",
      "    for metric in metrics:\n",
      "        dm_fp = join(bdiv_out_dir, '%s_otu_table_even%d.txt' % (metric, depth))\n",
      "        renamed_dm_fp = join(bdiv_out_dir, '%s_dm.txt' % metric)\n",
      "        !mv $dm_fp $renamed_dm_fp\n",
      "        for i in range(1, num_shuffled + 1):\n",
      "            renamed_dm_f = open(renamed_dm_fp, 'U')\n",
      "            shuffled_dm_fp = join(bdiv_out_dir, '%s_dm_shuffled%d.txt' % (metric, i))\n",
      "            shuffled_dm_f = open(shuffled_dm_fp, 'w')\n",
      "            shuffled_dm_f.write(shuffle_dm_fn(renamed_dm_f))\n",
      "            shuffled_dm_f.close()\n",
      "            renamed_dm_f.close()\n",
      "        \n",
      "        # Create subsampled distance matrices.\n",
      "        for category in categories:\n",
      "            for group_size in group_sizes:\n",
      "                for i in range(1, num_subsampled + 1):\n",
      "                    subsampled_dm_fp = join(bdiv_out_dir, '%s_dm_%s_ss%d_%d.txt' % (metric, category, group_size, i))\n",
      "                    subsampled_dm = open(subsampled_dm_fp, 'w')\n",
      "                    subsampled_dm.write(subsample_dm_fn(open(renamed_dm_fp, 'U'), open(map_fp, 'U'), category, group_size))\n",
      "                    subsampled_dm.close()\n",
      "\n",
      "# Process each depth in each study in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "!mkdir $out_dir\n",
      "per_study_depths = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        per_study_depths.append((study, depth, metrics, studies[study]['categories'], studies[study]['group_sizes'], num_shuffled, num_subsampled,\n",
      "                                 in_dir, out_dir, tree_fp, shuffle_dm, subsample_dm))\n",
      "out = dview.map(generate_per_study_depth_dms, per_study_depths)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Run grouping analysis methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run each *grouping analysis* statistical method on each distance matrix. These are methods that test out categorical variables such as sex, individual, body site, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import basename, exists, join, splitext\n",
      "from random import shuffle\n",
      "\n",
      "def run_compare_categories((method, category, permutation, dm_fp, map_fp, results_dir)):\n",
      "    !compare_categories.py --method $method -n $permutation -i $dm_fp -m $map_fp -c $category -o $results_dir\n",
      "\n",
      "jobs = []\n",
      "for study in studies:\n",
      "    for depth in studies[study]['depths']:\n",
      "        for method in grouping_methods:\n",
      "            study_dir = join(out_dir, study)\n",
      "            map_fp = join(study_dir, 'map.txt')\n",
      "            depth_dir = join(study_dir, 'bdiv_even%d' % depth)\n",
      "            dm_wildcard = join(depth_dir, '*_dm*.txt')\n",
      "            dm_fps = !ls $dm_wildcard\n",
      "            \n",
      "            for dm_fp in dm_fps:\n",
      "                for category in studies[study]['categories']:\n",
      "                    for permutation in permutations:\n",
      "                        dm_bn = basename(dm_fp)\n",
      "                        if 'dm.txt' in dm_bn or 'dm_shuffled' in dm_bn or 'dm_%s_ss' % category in dm_bn:\n",
      "                            results_dir = join(depth_dir, '%s_%s_%s_%d' % (splitext(dm_bn)[0], method, category, permutation))\n",
      "                            \n",
      "                            # Skip the job if the results dir exists and is not empty. We'll assume it was created from a previous run.\n",
      "                            if not exists(results_dir) or len(listdir(results_dir)) == 0:\n",
      "                                jobs.append((method, category, permutation, dm_fp, map_fp, results_dir))\n",
      "\n",
      "# Process compare_category.py runs in parallel.\n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "\n",
      "# We shuffle the jobs to (hopefully) get spread out the longer-running jobs over IPython engines (e.g. whole body @ 999 permutations).\n",
      "shuffle(jobs)\n",
      "out = dview.map(run_compare_categories, jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "1238"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Collate results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parse and collect the effect size statistics and p-values from each of the tests that were run. The resulting data structure can then be used to format result tables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = {}\n",
      "for depth_idx, depth_desc in enumerate(depth_descs):\n",
      "    depth_res = {}\n",
      "    for metric in metrics:\n",
      "        metric_res = {}\n",
      "        for method, res_parsing_fn in grouping_methods.items():\n",
      "            method_res = {}\n",
      "            for study in studies:\n",
      "                study_res = {}\n",
      "                \n",
      "                # Figure out what our actual depth is for the study, and what subsampled group sizes we performed.\n",
      "                depth = studies[study]['depths'][depth_idx]\n",
      "                group_sizes = studies[study]['group_sizes']\n",
      "                \n",
      "                for category in studies[study]['categories']:\n",
      "                    category_res = {}\n",
      "                    full_ess = []\n",
      "                    full_p_vals = []\n",
      "                    shuff_ess = []\n",
      "                    shuff_p_vals = []\n",
      "                    \n",
      "                    # TODO This next part is pretty messy... it needs to get cleaned up.\n",
      "                    for permutation in permutations:\n",
      "                        # Collect results for full distance matrices.\n",
      "                        full_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_%s_%s_%d' % (metric, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                        full_es, full_p_val = res_parsing_fn(full_res_f)\n",
      "                        full_res_f.close()\n",
      "                        full_ess.append(full_es)\n",
      "                        full_p_vals.append(full_p_val)\n",
      "                        \n",
      "                        # Collect results for shuffled distance matrices.\n",
      "                        avg_shuff_ess = []\n",
      "                        avg_shuff_p_vals = []\n",
      "                        for shuff_num in range(1, num_shuffled + 1):\n",
      "                            shuff_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_shuffled%d_%s_%s_%d' % (metric, shuff_num, method, category, permutation), '%s_results.txt' % method), 'U')\n",
      "                            shuff_es, shuff_p_val = res_parsing_fn(shuff_res_f)\n",
      "                            shuff_res_f.close()\n",
      "                            avg_shuff_ess.append(shuff_es)\n",
      "                            avg_shuff_p_vals.append(shuff_p_val)\n",
      "                        shuff_ess.append(median(avg_shuff_ess))\n",
      "                        shuff_p_vals.append(median(avg_shuff_p_vals))\n",
      "                        \n",
      "                    if len(set(full_ess)) != 1 or len(set(shuff_ess)) != 1:\n",
      "                        raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                    for p_val in full_p_vals:\n",
      "                        if p_val < 0 or p_val > 1:\n",
      "                            raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                    for p_val in shuff_p_vals:\n",
      "                        if p_val < 0 or p_val > 1:\n",
      "                            raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                    category_res['full'] = (full_ess[0], full_p_vals)\n",
      "                    category_res['shuffled'] = (shuff_ess[0], shuff_p_vals)\n",
      "                        \n",
      "                    # Collect results for subsampled distance matrices.\n",
      "                    ss_ess = []\n",
      "                    ss_p_vals = []\n",
      "                    for group_size in group_sizes:\n",
      "                        gs_ess = []\n",
      "                        gs_p_vals = []\n",
      "                        \n",
      "                        for permutation in permutations:\n",
      "                            avg_ss_ess = []\n",
      "                            avg_ss_p_vals = []\n",
      "                            for ss_num in range(1, num_subsampled + 1):\n",
      "                                ss_res_f = open(join(out_dir, study, 'bdiv_even%d' % depth, '%s_dm_%s_ss%d_%d_%s_%s_%d' % (metric, category, group_size, ss_num, method, category, permutation),\n",
      "                                                     '%s_results.txt' % method), 'U')\n",
      "                                ss_es, ss_p_val = res_parsing_fn(ss_res_f)\n",
      "                                ss_res_f.close()\n",
      "                                avg_ss_ess.append(ss_es)\n",
      "                                avg_ss_p_vals.append(ss_p_val)\n",
      "                            gs_ess.append(median(avg_ss_ess))\n",
      "                            gs_p_vals.append(median(avg_ss_p_vals))\n",
      "                        \n",
      "                        if len(set(gs_ess)) != 1:\n",
      "                            raise ValueError(\"The effect size statistics were not the same for different numbers of permutations. Something went wrong...\")\n",
      "                        for p_val in gs_p_vals:\n",
      "                            if p_val < 0 or p_val > 1:\n",
      "                                raise ValueError(\"Encountered invalid p-value: %.4f\" % p_val)\n",
      "                        ss_ess.append(gs_ess[0])\n",
      "                        ss_p_vals.append(gs_p_vals)\n",
      "                    \n",
      "                    if len(ss_ess) != len(ss_p_vals):\n",
      "                        raise ValueError(\"We don't have the same number of effect size statistics as p-values. Something went wrong...\")\n",
      "                    category_res['subsampled'] = (ss_ess, ss_p_vals)\n",
      "                    \n",
      "                    study_res[category] = category_res\n",
      "                method_res[study] = study_res\n",
      "            metric_res[method] = method_res\n",
      "        depth_res[metric] = metric_res\n",
      "    results[depth_desc] = depth_res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Sample size testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test the methods on subsets of the original sample groups. Samples will be chosen randomly without replacement to generate groups of samples at the specified subset size. A plot will be generated with subset size on the x-axis and test statistic on the y-axis. This will allow us to see if there is a cutoff/threshold based on the number of samples (i.e. where things start to stabilize in terms of the number of samples)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "from math import ceil\n",
      "from os.path import join\n",
      "from random import randint, shuffle\n",
      "from matplotlib.pyplot import errorbar, figure, legend, title, xlabel, xlim, ylabel\n",
      "from numpy import mean, std\n",
      "from qiime.filter import filter_samples_from_distance_matrix\n",
      "from qiime.parse import parse_distmat, parse_mapping_file_to_dict\n",
      "\n",
      "def run_cmd(cmd):\n",
      "    !$cmd\n",
      "\n",
      "group_sizes = [5, 10, 20, 40, 60, 80]\n",
      "num_subsets = 10\n",
      "study = 'whole_body'\n",
      "categories = {'BODY_SITE': ['b', 'Body site'], 'SEX': ['r', 'Sex']}\n",
      "even_depth = 575\n",
      "out_dir = '%s_grouping_analysis' % study\n",
      "grouping_methods = {\n",
      "                    'adonis': parse_adonis_results,\n",
      "                    'anosim': parse_anosim_permanova_results,\n",
      "                    'mrpp': parse_mrpp_results,\n",
      "                    'permanova': parse_anosim_permanova_results,\n",
      "                    'dbrda': parse_dbrda_results\n",
      "                   }\n",
      "\n",
      "map_fp = 'grouping_analysis_output/%s/map.txt' % study\n",
      "num_perms = 999"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mkdir $out_dir\n",
      "cmds = []\n",
      "for category in categories:\n",
      "    for group_size in group_sizes:\n",
      "        for subset_num in range(1, num_subsets + 1):\n",
      "            map_f = open(map_fp, 'U')\n",
      "            dm_f = open('grouping_analysis_output/%s/bdiv_even%d/unweighted_unifrac_dm.txt' % (study, even_depth), 'U')\n",
      "            subset_dm_fp = join(out_dir, 'unweighted_unifrac_dm_%s_ss%d_%d.txt' % (category, group_size, subset_num))\n",
      "            subset_dm_f = open(subset_dm_fp, 'w')\n",
      "            subset_dm_f.write(subsample_dm(dm_f, map_f, category, group_size))\n",
      "            subset_dm_f.close()\n",
      "            dm_f.close()\n",
      "            map_f.close()\n",
      "            \n",
      "            for method in grouping_methods:\n",
      "                results_dir = join(out_dir, 'unweighted_unifrac_dm_%s_ss%d_%d_%s' % (category, group_size, subset_num, method))\n",
      "                cmds.append('compare_categories.py --method %s -n %d -i %s -m %s -c %s -o %s' % (method, num_perms, subset_dm_fp, map_fp, category, results_dir))\n",
      "                \n",
      "c = Client()\n",
      "dview = c[:]\n",
      "dview.block = True\n",
      "shuffle(cmds)\n",
      "out = dview.map(run_cmd, cmds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for method, parse_fn in grouping_methods.items():\n",
      "    fig = figure()\n",
      "    ax1 = fig.add_subplot(111)\n",
      "    ax2 = ax1.twinx()\n",
      "\n",
      "    for category, plot_options in categories.items():\n",
      "        avg_test_stats = []\n",
      "        std_test_stats = []\n",
      "        avg_p_vals = []\n",
      "        std_p_vals = []\n",
      "        \n",
      "        for group_size in group_sizes:\n",
      "            test_stats = []\n",
      "            p_vals = []\n",
      "            \n",
      "            for subset_num in range(1, num_subsets + 1):\n",
      "                results_dir = join(out_dir, 'unweighted_unifrac_dm_%s_ss%d_%d_%s' % (category, group_size, subset_num, method))\n",
      "                test_stat, p_val = parse_fn(open(join(results_dir, '%s_results.txt' % method), 'U'))\n",
      "                test_stats.append(test_stat)\n",
      "                p_vals.append(p_val)\n",
      "                \n",
      "            avg_test_stats.append(mean(test_stats))\n",
      "            std_test_stats.append(std(test_stats))\n",
      "            avg_p_vals.append(mean(p_vals))\n",
      "            std_p_vals.append(std(p_vals))\n",
      "            \n",
      "        # Plot test statistics on left axis.\n",
      "        ax1.errorbar(group_sizes, avg_test_stats, yerr=std_test_stats, color=plot_options[0], label=plot_options[1], fmt='-')\n",
      "        \n",
      "        # Plot p-values on the right axis.\n",
      "        ax2.errorbar(group_sizes, avg_p_vals, yerr=std_p_vals, color=plot_options[0], label=plot_options[1], fmt='-', linestyle='--')\n",
      "    \n",
      "    xlim(0, 85)\n",
      "    title('%s: %s' % (study, method))\n",
      "    ax1.set_xlabel('Samples per group')\n",
      "    ax1.set_ylabel('Average test statistic with standard deviation')\n",
      "    ax2.set_ylabel('Average p-value with standard deviation')\n",
      "    legend()\n",
      "    fig.savefig(join(out_dir, 'grouping_analysis_plot_%s_%s.pdf' % (study, method)), format='pdf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    }
   ],
   "metadata": {}
  }
 ]
}